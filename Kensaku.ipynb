{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kensaku",
      "provenance": [],
      "authorship_tag": "ABX9TyMOK8qf9m8r/2nYxsACKRGw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfterMaturi/KensakuGijutsu/blob/main/Kensaku.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.データ**\n",
        "\n",
        "  青空文庫の文章データをURLからファイルを開きダウンロードして使用する。"
      ],
      "metadata": {
        "id": "C16dZZcuE_cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.分析手法、**\n",
        "\n",
        "ward2vecを用いて夏目漱石、太宰治、芥川龍之介、森鷗外の小説用いてを類似単語を調べる。\n",
        "ward2vecとは自然言語処理の手法で、文章中の単語を意味を示す数値ベクトルに変換し、その意味を把握するものである。\n",
        "最も小説を処理することに適しているので、この手法を選択する。"
      ],
      "metadata": {
        "id": "8HCypByQE_K1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne0JOG0U9x_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e5563a-f3bb-4f10-d88a-0469f709818e"
      },
      "source": [
        "#1:インポート\n",
        "import re\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import os.path\n",
        "import glob\n",
        "\n",
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7\n",
        "\n",
        "import MeCab\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "aptitude is already the newest version (0.8.10-6ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.12)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.19)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3ubuntu0.1)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.12)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.19)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3ubuntu0.1)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "No packages will be installed, upgraded, or removed.\n",
            "0 packages upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 0 B of archives. After unpacking 0 B will be used.\n",
            "                            \n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mecab-python3==0.7 in /usr/local/lib/python3.7/dist-packages (0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome\n",
        "from janome.tokenizer import Tokenizer\n",
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-y6gguOJx0T",
        "outputId": "0dee9a01-0437-4824-f1be-e5217a1d1a95"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.7/dist-packages (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2:テキストファイルの読み込み\n",
        "\n",
        "#夏目漱石・『吾輩は猫である』『三四郎』『こころ』\n",
        "NS_URL1 = 'https://www.aozora.gr.jp/cards/000148/files/789_ruby_5639.zip'\n",
        "NS_URL2 =  'https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip'\n",
        "NS_URL3 ='https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip' "
      ],
      "metadata": {
        "id": "C0k3iYqIPiYF"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#太宰治・『人間失格』『津軽』『女生徒』\n",
        "DO_URL1 = 'https://www.aozora.gr.jp/cards/000035/files/301_ruby_5915.zip'\n",
        "DO_URL2 = 'https://www.aozora.gr.jp/cards/000035/files/2282_ruby_1996.zip'\n",
        "DO_URL3 = 'https://www.aozora.gr.jp/cards/000035/files/275_ruby_1532.zip' "
      ],
      "metadata": {
        "id": "HOni2vcbQAOO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#芥川龍之介・『羅生門』『歯車』『鼻』\n",
        "AR_URL1 = 'https://www.aozora.gr.jp/cards/000879/files/127_ruby_150.zip'\n",
        "AR_URL2 = 'https://www.aozora.gr.jp/cards/000879/files/42377_ruby_34744.zip'\n",
        "AR_URL3 = 'https://www.aozora.gr.jp/cards/000879/files/42_ruby_154.zip'"
      ],
      "metadata": {
        "id": "aJUiZnZhMWsZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#森鷗外・『舞姫』『高瀬舟』『ヰタ・セクスアリス』\n",
        "MO_URL1='https://www.aozora.gr.jp/cards/000129/files/58126_ruby_73643.zip'\n",
        "MO_URL2='https://www.aozora.gr.jp/cards/000129/files/45245_ruby_21882.zip'\n",
        "MO_URL3='https://www.aozora.gr.jp/cards/000129/files/695_ruby_22805.zip'"
      ],
      "metadata": {
        "id": "GvnBAXjpUPJV"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3ドキュメント取得\n",
        "def download(URL):\n",
        "    # zipファイルのダウンロード\n",
        "    zip_file = re.split(r'/', URL)[-1]\n",
        "    urllib.request.urlretrieve(URL, zip_file)\n",
        "    dir = os.path.splitext(zip_file)[0]\n",
        "\n",
        "    # zipファイルの解凍と保存\n",
        "    with zipfile.ZipFile(zip_file) as zip_object:\n",
        "        zip_object.extractall(dir)\n",
        "\n",
        "    os.remove(zip_file)\n",
        "\n",
        "    # 保存したファイルのパスを取得\n",
        "    path = os.path.join(dir,'*.txt')\n",
        "    list = glob.glob(path)\n",
        "    return list[0]"
      ],
      "metadata": {
        "id": "L9MANC2hCrhZ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4文の抽出・整形\n",
        "def convert(download_text):\n",
        "    # ファイル読み込み\n",
        "    data = open(download_text, 'rb').read()\n",
        "    text = data.decode('shift_jis')\n",
        "\n",
        "    # 本文の抽出\n",
        "    text = re.split(r'\\-{5,}', text)[2]  \n",
        "    text = re.split(r'底本：', text)[0]\n",
        "    text = re.split(r'［＃改ページ］', text)[0]\n",
        "\n",
        "    # ノイズ削除\n",
        "    text = re.sub(r'《.+?》', '', text)\n",
        "    text = re.sub(r'［＃.+?］', '', text)\n",
        "    text = re.sub(r'｜', '', text)\n",
        "    text = re.sub(r'\\r\\n', '', text)\n",
        "    text = re.sub(r'\\u3000', '', text)  \n",
        "    text = re.sub(r'「', '', text) \n",
        "    text = re.sub(r'」', '', text)\n",
        "    text = re.sub(r'、', '', text)\n",
        "    text = re.sub(r'。', '', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "ALbuuxmoHCeV"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = download(NS_URL1)\n",
        "NS_text1 = convert(file)\n",
        "\n",
        "file = download(NS_URL2)\n",
        "NS_text2 = convert(file)\n",
        "\n",
        "file = download(NS_URL3)\n",
        "NS_text3 = convert(file)\n",
        "\n",
        "NS_text = NS_text1 + NS_text2 + NS_text3"
      ],
      "metadata": {
        "id": "6s-0WPQJPwx9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = download(DO_URL1)\n",
        "DO_text1 = convert(file)\n",
        "\n",
        "file = download(DO_URL2)\n",
        "DO_text2 = convert(file)\n",
        "\n",
        "file = download(DO_URL3)\n",
        "DO_text3 = convert(file)\n",
        "\n",
        "DO_text = DO_text1 + DO_text2 + DO_text3"
      ],
      "metadata": {
        "id": "0KMwdsujQe4v"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = download(AR_URL1)\n",
        "AR_text1 = convert(file)\n",
        "\n",
        "file = download(AR_URL2)\n",
        "AR_text2 = convert(file)\n",
        "\n",
        "file = download(AR_URL3)\n",
        "AR_text3 = convert(file)\n",
        "\n",
        "AR_text = AR_text1 + AR_text2 + AR_text3"
      ],
      "metadata": {
        "id": "MIteN4rNQiX-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = download(MO_URL1)\n",
        "MO_text1 = convert(file)\n",
        "\n",
        "file = download(MO_URL2)\n",
        "MO_text2 = convert(file)\n",
        "\n",
        "file = download(MO_URL3)\n",
        "MO_text3 = convert(file)\n",
        "\n",
        "MO_text = MO_text1 + MO_text2 + MO_text3"
      ],
      "metadata": {
        "id": "UWVGf-O4VIJx"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 形態素解析 --- (※3)\n",
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = NS_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '夏目漱石b.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('夏目漱石b.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('夏目漱石b.model')\n",
        "\n",
        "for word in ['結婚', '幸','人生','友']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54xnK2XEIEVK",
        "outputId": "b39b14bb-eaf9-4f0c-a62b-0da6c74d6eed"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = 生まれつく,弁論,諾否,聞き及ぶ,差異,祝,穏当,不可能,予審,集散\n",
            "幸 = 訳合,にがい,夜毎,こだわり,むると,取りとめる,機能,瀾怒濤,意思,此種\n",
            "人生 = 進捗,口舌,争論,願望,嗜欲,ムード,閑生涯,有意義,厳粛,教訓\n",
            "友 = おくる,ヴァレリアス,江湖,揶揄,面前,せり,一眼,処士,もとづく,陋策\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = DO_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '太宰治b.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('太宰治b.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('太宰治b.model')\n",
        "\n",
        "for word in ['結婚', '成長', '幸','人生','友']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpgwKdhENZyr",
        "outputId": "c8387bcc-f356-4df4-c0f5-37f0459ceee0"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = 不承知,ときめかす,営む,生き抜く,内福,誤解,著述,鍛,世継,うしなう\n",
            "成長 = 困惑,ゆき,体,いたたまらない,よこしま,時時,理性,くるしむ,掴める,与太者\n",
            "幸 = 鼓腹,山の幸,撃壌,宿賃,別天地,山麓,ワラビゼンマイウドタケノコフキアザミキノコ,めぐまれる,新浜,出港\n",
            "人生 = 切実,受難,かがやかしい,指針,既婚,名案,懐疑,のる,片すみ,二目\n",
            "友 = セリヌンティウス,面前,竹馬,石工,きかせる,ディオニス,佳い,齧る,奸佞,宿す\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = AR_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '芥川龍之介b.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('芥川龍之介b.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('芥川龍之介b.model')\n",
        "\n",
        "for word in ['結婚','人生']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar978RdnIVNR",
        "outputId": "c4751344-18d4-411b-9bc0-7a8703d9df1d"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = 調節,性慾,有効,披露,相談,つらなる,晩餐,しかる,社,慰安\n",
            "人生 = オリムピツク,主催,大会,踏み入る,胎内,踏み止まる,創痍,泳げる,競技,闘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = MO_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '森鷗外b.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('森鷗外b.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('森鷗外b.model')\n",
        "\n",
        "for word in ['結婚', '幸','人生','友']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxEgWrRqVVob",
        "outputId": "7b755299-b1f1-427b-8c28-d2273e9e6490"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = dub,余計,秘密,霊,むつかしい,男女,穢い,兼,仲間はずれ,方角\n",
            "幸 = 年長,理想,鋭敏,疑惑,庄,二つ,ことわる,助,くい,退学\n",
            "人生 = 心理,有す,研究,情熱,発揮,萌芽,異常,象,状態,芸術\n",
            "友 = 報じる,嫉む,嘲る,出入り,のち,宣う,途,望み,彼ら,鏡\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "各文豪に対して、代表3作を用いて、類似単語を求めたが、それぞれ、類似と言うにはかけ離れていたり、作品の雰囲気を反映しすぎている。\n",
        "選択した語が登場しないので、分析することができないばあいもあった。\n",
        "\n",
        "精度を上げるために、それぞれ15作ほどを入力し、類似単語を調べる。"
      ],
      "metadata": {
        "id": "ZFu1vVmbPnwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#夏目漱石\n",
        "NS_URL = ['https://www.aozora.gr.jp/cards/000148/files/789_ruby_5639.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/752_ruby_2438.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/777_ruby_1717.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/776_ruby_6020.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/775_ruby_2064.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/767_ruby_3256.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/56143_ruby_50824.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/798_ruby_2413.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/56923_ruby_66668.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/783_ruby_1311.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/799_ruby_6024.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/1076_ruby_4527.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000148/files/780_ruby_569.zip']\n",
        "          \n",
        "for i in NS_URL:\n",
        "    download_file = download(i)\n",
        "    text = convert(download_file)\n",
        "    NS_text += text"
      ],
      "metadata": {
        "id": "RGknUQznPnTL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = NS_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '夏目漱石.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('夏目漱石.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('夏目漱石.model')\n"
      ],
      "metadata": {
        "id": "CjVjWi0jjzak"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#太宰治\n",
        "DO_URL = ['https://www.aozora.gr.jp/cards/000035/files/301_ruby_5915.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/2282_ruby_1996.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/275_ruby_1532.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/1574_ruby_15426.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/260_ruby_34636.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/258_ruby_20178.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/1567_ruby_4948.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/312_ruby_2924.zip', \n",
        "          'https://www.aozora.gr.jp/cards/000035/files/263_ruby_20160.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/307_ruby_3042.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/46597_ruby_24666.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/45675_ruby_20802.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/1565_ruby_8220.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/46599_ruby_24668.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000035/files/45676_ruby_20801.zip']\n",
        "\n",
        "for i in DO_URL:\n",
        "    download_file = download(i)\n",
        "    text = convert(download_file)\n",
        "    DO_text += text"
      ],
      "metadata": {
        "id": "pSAGKjiMRAB4"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = DO_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '太宰治.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('太宰治.model')\n",
        "\n"
      ],
      "metadata": {
        "id": "-lCBjS1rTEOD"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#芥川龍之介\n",
        "AR_URL = ['https://www.aozora.gr.jp/cards/000879/files/127_ruby_150.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/42377_ruby_34744.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/42_ruby_154.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/15_ruby_904.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/3804_ruby_27189.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/43017_ruby_17394.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/3803_ruby_27208.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/37_ruby_1265.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/3820_ruby_27216.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/60_ruby_821.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/43751_ruby_27860.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/155_ruby_1146.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/143_ruby_833.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/166_ruby_1459.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/170_ruby_348.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000879/files/1141_ruby_5097.zip'] \n",
        "\n",
        "for i in AR_URL:\n",
        "    download_file = download(i)\n",
        "    text = convert(download_file)\n",
        "    AR_text += text  "
      ],
      "metadata": {
        "id": "FBX64hzDkofb"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = AR_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '芥川龍之介.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('芥川龍之介.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('芥川龍之介.model')\n",
        "\n"
      ],
      "metadata": {
        "id": "OJLM-71dnChy"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#森鷗外\n",
        "MO_URL = ['https://www.aozora.gr.jp/cards/000129/files/58126_ruby_73643.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/45245_ruby_21882.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/695_ruby_22805.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/45224_ruby_19890.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/2522_ruby_5001.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/689_ruby_23256.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/673_ruby_23254.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/2058_ruby_19627.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/694_ruby_23249.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/43030_ruby_17344.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000356/files/2060_ruby_23189.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/1054_ruby_23199.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/2298_ruby_6609.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/2084_ruby_17360.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/3614_ruby_12061.zip',\n",
        "          'https://www.aozora.gr.jp/cards/000129/files/2082_ruby_23122.zip'] \n",
        "\n",
        "for i in MO_URL:\n",
        "    download_file = download(i)\n",
        "    text = convert(download_file)\n",
        "    MO_text += text"
      ],
      "metadata": {
        "id": "rBIeKIXPV5Vb"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "results = []\n",
        "# テキストを一行ずつ処理する\n",
        "lines = MO_text.split(\"\\r\\n\")\n",
        "for line in lines:\n",
        "    s = line\n",
        "    s = s.replace('｜', '')\n",
        "    s = re.sub(r'《.+?》', '', s) # ルビを削除\n",
        "    s = re.sub(r'［＃.+?］', '', s) # 入力注を削除\n",
        "    tokens = t.tokenize(s) # 形態素解析\n",
        "    # 必要な語句だけを対象とする --- (※4)\n",
        "    r = []\n",
        "    for tok in tokens:\n",
        "        if tok.base_form == \"*\": # 単語の基本系を採用\n",
        "            w = tok.surface\n",
        "        else:\n",
        "            w = tok.base_form\n",
        "        ps = tok.part_of_speech # 品詞情報\n",
        "        hinsi = ps.split(',')[0]\n",
        "        if hinsi in ['名詞', '形容詞', '動詞', '記号']:\n",
        "            r.append(w)\n",
        "    rl = (\" \".join(r)).strip()\n",
        "    results.append(rl)\n",
        "    #print(rl) # --- 画面に分かち書きした行を表示\n",
        "\n",
        "    # 書き込み先テキストを開く --- (※5)\n",
        "wakati_file = '森鷗外.wakati'\n",
        "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
        "    fp.write(\"\\n\".join(results))\n",
        "\n",
        "# Word2Vecでモデルを作成 --- (※6)\n",
        "data = word2vec.LineSentence(wakati_file)\n",
        "model = word2vec.Word2Vec(data, \n",
        "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
        "model.save('森鷗外.model')\n",
        "\n",
        "model = word2vec.Word2Vec.load('森鷗外.model')"
      ],
      "metadata": {
        "id": "sbvQDUqaV5yz"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.結果**"
      ],
      "metadata": {
        "id": "KoOkcpAZasTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "夏目漱石"
      ],
      "metadata": {
        "id": "_kri9Zmga0G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('夏目漱石.model')\n",
        "for word in ['結婚', '人生', '友人', '愛', '夢', '生きる', '他人']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZyoHa8TZ6DC",
        "outputId": "ec26d41b-eed9-41cf-9618-5dfed2267305"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = 生まれつく,集散,承諾,諾否,破約,予審,祝,判事,打明ける,臆測\n",
            "人生 = 進捗,争論,口舌,願望,観,嗜欲,有意義,閑生涯,原動力,暗示\n",
            "友人 = 奉職,だする,バルザック,告別,横浜,一面識,吐月峰,出帆,語り物,用達\n",
            "愛 = 芸妓,男性,都会人,カ条,権能,渝,方案,完,睦む,永遠\n",
            "夢 = うなされる,くるしい,冷汗,うずく,空模様,ダイヤモンド,慧,絶間,五彩,空山\n",
            "生きる = 曲淵,耶蘇孔,張付ける,理性,囁く,誨,ちょん髷,慈悲,いるか,かこつ\n",
            "他人 = 傾城,りょう,推して知るべし,親身,朝晩,容喙,あか,妄り,慈愛,敵討\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('夏目漱石.model')\n",
        "\n",
        "iwords = ['他人','社会','人生' ]\n",
        "words = model.most_similar(positive=iwords)\n",
        "n = [w[0] for w in words]\n",
        "print(words,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IHzWmkmZ54o",
        "outputId": "04317f02-2718-4215-e6fe-f2638a19326f"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('切り捨てる', 0.7673225998878479), ('原動力', 0.7617892026901245), ('切り捨て', 0.7449092864990234), ('たかれる', 0.7423896193504333), ('有意義', 0.7422025799751282), ('天分', 0.7308719754219055), ('本末', 0.7267889976501465), ('口舌', 0.7266632914543152), ('今夕', 0.7227792739868164), ('暗示', 0.7209005355834961)] = 切り捨てる,原動力,切り捨て,たかれる,有意義,天分,本末,口舌,今夕,暗示\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "太宰治"
      ],
      "metadata": {
        "id": "SVu7h6pBa3BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('太宰治.model')\n",
        "for word in ['結婚', '人生', '友人', '愛', '夢', '生きる', '他人']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH4UhgW1Z5pF",
        "outputId": "6edd8603-2d74-4ad5-a64e-433c35df8e7b"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = 不承知,営む,内福,ときめかす,誤解,初婚,世継,生き抜く,推す,著述\n",
            "人生 = 指針,既婚,切実,かがやかしい,受難,片すみ,二目,のる,懐疑,典雅\n",
            "友人 = 八つ手,ピクニツク,でかける,承認,けち,仲直り,散髪,卑怯,構える,辛抱\n",
            "愛 = 禽家,禽,誠,誤解,模倣,偉大,とぼしい,血液,サタン,独創\n",
            "夢 = 五臓,離れ島,仮寝,エヘッ,きだ,空しい,醒める,つづ,慕,うなされる\n",
            "生きる = 生きのびる,死ねる,成り行き,考え方,定法,栄冠,たえる,朗らか,事業,無くす\n",
            "他人 = 真顔,思いやる,心持,難易,恋しい,監獄,ゆき,悩む,報恩,不可解\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('太宰治.model')\n",
        "\n",
        "iwords = ['他人','社会','人生' ]\n",
        "words = model.most_similar(positive=iwords)\n",
        "n = [w[0] for w in words]\n",
        "print(words,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2vud-ZOZ3sg",
        "outputId": "b7c342b8-0798-43f7-fe06-cbb1495b7dc4"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('真顔', 0.667510986328125), ('のる', 0.6629472374916077), ('転嫁', 0.6621818542480469), ('切実', 0.6575527191162109), ('既婚', 0.6326800584793091), ('二目', 0.6281033754348755), ('高慢ちき', 0.619822084903717), ('無頼', 0.6193315386772156), ('三目', 0.6191238164901733), ('知ったかぶり', 0.609961211681366)] = 真顔,のる,転嫁,切実,既婚,二目,高慢ちき,無頼,三目,知ったかぶり\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "芥川龍之介"
      ],
      "metadata": {
        "id": "IRbxTDxia4tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('芥川龍之介.model')\n",
        "for word in ['結婚', '人生', '友人', '愛', '夢', '生きる', '他人']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRcjL9wdZum5",
        "outputId": "4b778a38-215c-478e-ba24-f81d4f6317dc"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = 調節,披露,有効,性慾,飛びこむ,つらなる,相談,式,庭前,古池\n",
            "人生 = 踏み入る,泳げる,主催,大会,創痍,競技,オリムピツク,胎内,処する,踏み止まる\n",
            "友人 = 金持ち,甲,児女,乙,親密,痛痒,未開,人種,打ち倒す,虚偽\n",
            "愛 = 憐憫,適す,一半,公,高尚,悪党,必,神聖,養育,利己\n",
            "夢 = 蓄,怒り,獅子王,革命,億,いく,薄れる,哩,風の音,仄めく\n",
            "生きる = ゆく,枯らす,うせる,あたり前,未練,思える,釈,苦しい,現在,ほど\n",
            "他人 = かつ,敏感,談,傍若無人,俗人,グルモン,一半,絶縁,名士,鹹水\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('芥川龍之介.model')\n",
        "\n",
        "iwords = ['他人','社会','人生' ]\n",
        "words = model.most_similar(positive=iwords)\n",
        "n = [w[0] for w in words]\n",
        "print(words,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfuuwbVJZwu6",
        "outputId": "164ee311-f1b2-4362-ab64-486234327eb7"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('石器', 0.8750275373458862), ('鋭敏', 0.8633654117584229), ('終止', 0.8608265519142151), ('極度', 0.8605843782424927), ('ベエメ', 0.8572107553482056), ('スウエデンボルグ', 0.8549501895904541), ('衰退', 0.8534131050109863), ('脳髄', 0.8521925806999207), ('亡国', 0.8503350615501404), ('長足', 0.8472084999084473)] = 石器,鋭敏,終止,極度,ベエメ,スウエデンボルグ,衰退,脳髄,亡国,長足\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "森鷗外"
      ],
      "metadata": {
        "id": "Kt3HUTZRa6Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('森鷗外.model')\n",
        "for word in ['結婚', '人生', '友人', '愛', '夢', '生きる', '他人']:\n",
        "    words = model.most_similar(positive=[word])\n",
        "    n = [w[0] for w in words]\n",
        "    print(word,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vma3rlglV6No",
        "outputId": "d4d1e20c-438b-41ed-d757-0525ffe980df"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結婚 = dub,余計,考え直す,疎む,落第,枯らす,騎士,卑賤,択ぶ,多言\n",
            "人生 = 日常,Apollon,利他,押し広める,色調,飛躍,忠義,外れる,利己,social\n",
            "友人 = 其墓,在京,其婚姻,其年,竜華,二本松,興る,淇園,往訪,其伝記\n",
            "愛 = 憂,没,是清,阿武,逸書,滝の川,満堂,堪る,吟行,西土\n",
            "夢 = 浮ぶ,美しい,過ぎ去る,梅暦,見掛ける,人情本,奩体,縁遠い,蝦蟇,治郎\n",
            "生きる = 機縁,復讐,無くなる,専売,アンドレア・バルヂピエロ,厄介,老衰,過度,迫害,すねる\n",
            "他人 = 決断,制する,順境,感慨,かかわる,通達,身一つ,侮る,けんか,美貌\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = word2vec.Word2Vec.load('森鷗外.model')\n",
        "\n",
        "iwords = ['他人','社会','人生' ]\n",
        "words = model.most_similar(positive=iwords)\n",
        "n = [w[0] for w in words]\n",
        "print(words,\"=\", \",\".join(n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdSrSe3gV6gp",
        "outputId": "939abdd4-f944-44cb-f922-57250b1f2e19"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('肯定', 0.8774096965789795), ('ルソオ', 0.871634304523468), ('social', 0.8628804087638855), ('政策', 0.8583863973617554), ('忠義', 0.857492983341217), ('万有', 0.853191614151001), ('人工', 0.8511061668395996), ('訐', 0.8488544225692749), ('浅薄', 0.8481053113937378), ('Atavisme', 0.8436183929443359)] = 肯定,ルソオ,social,政策,忠義,万有,人工,訐,浅薄,Atavisme\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.考察**\n",
        "\n",
        "四者の作風が垣間見えるようになり、その違いが浮かび上がるようになった。\n",
        "他人に対する類似単語からその違いが最も分かる。\n",
        "\n",
        "夏目漱石：傾城,りょう,推して知るべし,親身,朝晩,容喙,あか,妄り,慈愛,敵討\n",
        "\n",
        "太宰治：真顔,思いやる,心持,難易,恋しい,監獄,ゆき,悩む,報恩,不可解\n",
        "\n",
        "芥川龍之介：かつ,敏感,談,傍若無人,俗人,グルモン,一半,絶縁,名士,鹹水\n",
        "\n",
        "森鷗外：決断,制する,順境,感慨,かかわる,通達,身一つ,侮る,けんか,美貌\n",
        "\n",
        "夏目漱石は親身、慈愛など、有効な間柄であるとともに敵討という不穏なものも孕んでいる。\n",
        "\n",
        "太宰治は不可解、難易、監獄、悩むと、他人を理解できない心情が表れている。\n",
        "\n",
        "芥川龍之介は敏感、傍若無人、俗人、絶縁と、他人と相容れない人間であることが分かる。\n",
        "\n",
        "森鷗外は他人を競争相手であったり、敵であるという考えがあるように思える。\n",
        "\n",
        "しかし、どれも類似単語とは言えそうもない単語が半分程度ある。一作品の影響を、まだまだ強く受けているので、より作品数を増やさなければ、正確には捉えられない。\n"
      ],
      "metadata": {
        "id": "ll5IFoXbr-Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考文献\n",
        "https://qiita.com/y_itoh/items/1e91ec59dac403d8e8e6　：　Pythonによる自然言語処理\n"
      ],
      "metadata": {
        "id": "_Ga8XWts_Oz5"
      }
    }
  ]
}